#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
123ç½‘ç›˜æœåŠ¡ï¼ˆç²¾ç®€ç‰ˆï¼‰
ä»…é€šè¿‡â€œè‡ªå®šä¹‰åŸŸå + æ˜ å°„è·¯å¾„ç›´å‡º + URLé‰´æƒâ€è·å–ç›´é“¾
"""

import os
import re
import time
import logging
import hashlib
import requests

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# å¦‚æœæ²¡æœ‰å¤„ç†å™¨ï¼Œæ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

from utils.url_auth import URLAuthManager
from utils.cache import CacheManager


class Pan123Service:
    """123ç½‘ç›˜æœåŠ¡"""
    
    def __init__(self, client, config):
        self.client = client
        self.config = config
        self.auth_manager = URLAuthManager()
        self.cache = CacheManager()
    
    def get_file_direct_link(self, file_name, mapped_path=None):
        """
        è·å–æ–‡ä»¶ç›´é“¾ï¼ˆæ”¯æŒç›´é“¾æ¨¡å¼å’Œä»£ç†æ¨¡å¼åˆ‡æ¢ï¼‰
        
        æµç¨‹ï¼š
        1. æ£€æŸ¥ä¸‹è½½æ¨¡å¼è®¾ç½®
        2. ç›´é“¾æ¨¡å¼ï¼šå°è¯•è‡ªå®šä¹‰åŸŸåç›´å‡º + URLé‰´æƒ
        3. ä»£ç†æ¨¡å¼ï¼šé€šè¿‡APIè·å–ä¸‹è½½é“¾æ¥ + ä»£ç†å¤„ç†
        
        :param file_name: æ–‡ä»¶å
        :param mapped_path: æ˜ å°„åçš„è·¯å¾„
        :return: æ–‡ä»¶ä¿¡æ¯ï¼ˆåŒ…å«raw_urlï¼‰
        """
        
        # å¿…é¡»æä¾›æ˜ å°„åçš„ç½‘ç›˜è·¯å¾„
        if not mapped_path:
            return None

        # æ£€æŸ¥ä¸‹è½½æ¨¡å¼è®¾ç½®
        download_mode = self.config.get('123', {}).get('download_mode', 'direct')

        # ç¬¬0æ­¥ï¼šå‘½ä¸­ç›´é“¾çŸ­æœŸç¼“å­˜ï¼ˆæŒ‰æ˜ å°„è·¯å¾„ç¼“å­˜ï¼Œé¿å…åŒåå†²çªï¼‰
        # æ³¨æ„ï¼šç›´é“¾æ¨¡å¼ï¼ˆåŸŸå+è·¯å¾„ï¼‰ä¸ç¼“å­˜ï¼Œä»£ç†æ¨¡å¼æ‰ç¼“å­˜ï¼ˆé¿å…é¢‘ç¹APIæŸ¥è¯¢ï¼‰
        # æ·»åŠ ç‰ˆæœ¬æ ‡è¯†ï¼Œç¡®ä¿ç­¾åé€»è¾‘æ›´æ–°åä¸ä½¿ç”¨æ—§ç¼“å­˜
        cache_key = f"123:{mapped_path}:{download_mode}:v2"
        
        # åªæœ‰ä»£ç†æ¨¡å¼æ‰ä½¿ç”¨ç¼“å­˜
        if download_mode == 'proxy' and self.cache.is_direct_link_valid(cache_key):
            cached = self.cache.get_direct_link(cache_key) or {}
            url = cached.get('url')
            if url:
                logger.info(f"âš¡ ä»£ç†ç›´é“¾ç¼“å­˜å‘½ä¸­: {file_name}")
                return {
                    'name': file_name,
                    'size': 0,
                    'is_dir': False,
                    'modified': '',
                    'raw_url': url,
                    'sign': '',
                    'header': {}
                }

        # æ ¹æ®ä¸‹è½½æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
        if download_mode == 'proxy':
            result = self._get_proxied_download_link(file_name, mapped_path)
            if result:
                # ç¼“å­˜ä»£ç†ä¸‹è½½é“¾æ¥
                try:
                    self.cache.set_direct_link(cache_key, result['raw_url'], expire_time=300)
                except Exception:
                    pass
            return result
        
        elif download_mode == 'direct':
            try:
                # ç¬¬ä¸€æ­¥ï¼šè‡ªå®šä¹‰åŸŸå + è·¯å¾„ç›´å‡º
                if not self._can_build_from_domain_path():
                    logger.warning("âš ï¸ æœªå¯ç”¨URLé‰´æƒæˆ–æœªé…ç½®è‡ªå®šä¹‰åŸŸåï¼Œé™çº§åˆ°ä»£ç†ä¸‹è½½")
                    return self._get_proxied_download_link(file_name, mapped_path)
                
                direct_url = self._build_url_from_domain_and_path(mapped_path)
                if not direct_url:
                    logger.warning("âš ï¸ åŸŸåç›´å‡ºå¤±è´¥ï¼Œé™çº§åˆ°ä»£ç†ä¸‹è½½")
                    return self._get_proxied_download_link(file_name, mapped_path)

                # ç¬¬äºŒæ­¥ï¼šæ·»åŠ URLé‰´æƒ
                direct_url = self._add_url_auth(direct_url)
                
                # ç¬¬ä¸‰æ­¥ï¼šå¿«é€Ÿç›´é“¾éªŒè¯ï¼ˆä¼˜åŒ–è¶…æ—¶æ—¶é—´ï¼‰
                if self._quick_validate_direct_url(direct_url):
                    logger.info(f"âœ… ç›´è¿éªŒè¯æˆåŠŸ: {file_name}")
                else:
                    logger.warning(f"âš ï¸ ç›´è¿éªŒè¯å¤±è´¥ï¼Œå¿«é€Ÿé™çº§åˆ°ä»£ç†ä¸‹è½½")
                    return self._get_fast_proxied_download_link(file_name, mapped_path)

                # ç¬¬å››æ­¥ï¼šç›´é“¾æ¨¡å¼ä¸éœ€è¦ç¼“å­˜ï¼ˆåŸŸå+è·¯å¾„æ„å»ºå¾ˆå¿«ï¼‰
                # åªåœ¨ä¸Šå±‚Embyä»£ç†ä¸­ç¼“å­˜æœ€ç»ˆç»“æœï¼Œé¿å…é‡å¤æŸ¥è¯¢Emby API
                # æ³¨é‡Šï¼šä»£ç†æ¨¡å¼æ‰éœ€è¦ç¼“å­˜ï¼Œå› ä¸ºéœ€è¦APIæŸ¥è¯¢
                
                logger.debug(f"ğŸ”— ç›´é“¾æ¨¡å¼æˆåŠŸ: {file_name}")
                
                # è¿”å›å®Œæ•´ä¿¡æ¯
                return {
                    'name': file_name,
                    'size': 0,
                    'is_dir': False,
                    'modified': '',
                    'raw_url': direct_url,
                    'sign': '',
                    'header': {}
                }
                
            except Exception as e:
                logger.error(f"âŒ ç›´é“¾æ¨¡å¼å¼‚å¸¸: {e}ï¼Œé™çº§åˆ°ä»£ç†ä¸‹è½½")
                return self._get_proxied_download_link(file_name, mapped_path)
        
        else:
            logger.error(f"âŒ ä¸æ”¯æŒçš„ä¸‹è½½æ¨¡å¼: {download_mode}")
            return None

    def _get_proxied_download_link(self, file_name, mapped_path=None, use_cache=True):
        """
        é€šè¿‡ä»£ç†æ–¹å¼è·å–ä¸‹è½½é“¾æ¥ï¼ˆæ”¯æŒç¼“å­˜æ§åˆ¶ï¼‰
        
        :param file_name: æ–‡ä»¶å
        :param mapped_path: æ˜ å°„åçš„è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        :return: æ–‡ä»¶ä¿¡æ¯ï¼ˆåŒ…å«ä»£ç†åçš„ä¸‹è½½é“¾æ¥ï¼‰
        """
        try:
            # æå–æ–‡ä»¶åä¸­çš„æ–‡ä»¶IDæˆ–å…¶ä»–æ ‡è¯†
            file_id_match = re.search(r'\[(\d+)\]', file_name)
            file_id = file_id_match.group(1) if file_id_match else None

            # å¦‚æœæ²¡æœ‰æ–‡ä»¶IDï¼Œå°è¯•æœç´¢è·å–
            if not file_id:
                search_result = self.client.fs_list_new({
                    'SearchData': file_name,
                    'limit': 1
                })
                
                if search_result and search_result.get('code') == 0:
                    items = search_result.get('data', {}).get('InfoList', [])
                    if items:
                        file_id = items[0].get('FileId')

            if not file_id:
                logger.error(f"âŒ æ— æ³•è·å–æ–‡ä»¶ID: {file_name}")
                return None

            # è·å–ä¸‹è½½é“¾æ¥
            download_url = self.client.download_url({'FileID': file_id})
            
            if not download_url:
                logger.error(f"âŒ è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: {file_name}")
                return None

            # é€šè¿‡ä»£ç†æ–¹å¼å¤„ç†ä¸‹è½½é“¾æ¥
            proxied_url = self._proxy_download_url(download_url)

            # ä¸ç¼“å­˜ä»£ç†é“¾æ¥ï¼Œå› ä¸ºä»£ç†é“¾æ¥å®¹æ˜“å¤±æ•ˆ
            # ä»£ç†é“¾æ¥é€šè¿‡æœåŠ¡å™¨è½¬å‘ï¼Œé“¾æ¥æœ¬èº«ä¸ä¼šå¤±æ•ˆï¼Œä½†å†…å®¹é“¾æ¥å¯èƒ½å¤±æ•ˆ
            # ä¸ºäº†ç¡®ä¿è·å–æœ€æ–°çš„ä¸‹è½½é“¾æ¥ï¼Œä¸ç¼“å­˜ä»£ç†URL
            logger.debug(f"ğŸ“ ä»£ç†é“¾æ¥ä¸ç¼“å­˜ï¼Œç¡®ä¿è·å–æœ€æ–°ä¸‹è½½é“¾æ¥")

            return {
                'name': file_name,
                'size': 0,
                'is_dir': False,
                'modified': '',
                'raw_url': proxied_url,
                'sign': '',
                'header': {}
            }

        except Exception as e:
            logger.error(f"âŒ ä»£ç†ä¸‹è½½é“¾æ¥è·å–å¼‚å¸¸: {e}")
            return None

    def _proxy_download_url(self, download_url):
        """
        å¯¹ä¸‹è½½é“¾æ¥è¿›è¡Œä»£ç†å¤„ç† - è¿”å›å®Œæ•´çš„ä»£ç†URL
        
        :param download_url: åŸå§‹ä¸‹è½½é“¾æ¥
        :return: ä»£ç†åçš„ä¸‹è½½é“¾æ¥ï¼ˆå®Œæ•´URLï¼‰
        """
        try:
            from urllib.parse import quote
            
            # âš ï¸ å…³é”®ï¼šå¯¹åŸå§‹ä¸‹è½½é“¾æ¥è¿›è¡ŒURLç¼–ç ï¼Œé¿å…å‚æ•°æ··æ·†
            encoded_url = quote(download_url, safe='')
            
            # åŠ¨æ€è¯»å–Embyåå‘ä»£ç†ç«¯å£é…ç½®
            emby_config = self.config.get('emby', {})
            service_config = self.config.get('service', {})
            
            # ä»é…ç½®ä¸­åŠ¨æ€è·å–Embyä»£ç†ç«¯å£ï¼Œå…è®¸ç”¨æˆ·è‡ªå®šä¹‰
            emby_proxy_port = emby_config.get('port', 8096)
            port = emby_proxy_port
            
            logger.debug(f"ğŸ“¡ åŠ¨æ€è¯»å–Embyä»£ç†ç«¯å£: {port}")
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„å¤–éƒ¨è®¿é—®åœ°å€ï¼ˆç”¨äºä»£ç†æ¨¡å¼ï¼‰
            external_url = service_config.get('external_url', '')
            
            if external_url:
                # ä½¿ç”¨é…ç½®çš„å¤–éƒ¨è®¿é—®åœ°å€ï¼ˆæ¨èï¼‰
                # ä¾‹å¦‚: http://your-server.com:5245 æˆ– https://your-domain.com
                base_url = external_url.rstrip('/')
                proxied_url = f"{base_url}/proxy/download?url={encoded_url}"
                logger.info(f"ğŸ”„ ä½¿ç”¨å¤–éƒ¨åœ°å€ç”Ÿæˆä»£ç†URL: {proxied_url[:80]}...")
            else:
                # å¦‚æœæ²¡æœ‰é…ç½®å¤–éƒ¨åœ°å€ï¼Œä½¿ç”¨è¯·æ±‚å¤´ä¸­çš„Hostï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
                try:
                    from flask import request
                    if request:
                        # ä»å½“å‰è¯·æ±‚è·å–å®¢æˆ·ç«¯è®¿é—®çš„åœ°å€
                        scheme = 'https' if request.is_secure else 'http'
                        host = request.host  # åŒ…å«ç«¯å£ï¼Œå¦‚ dy.127255.best:8096
                        # ä½¿ç”¨Embyä»£ç†ç«¯å£
                        if ':' in host:
                            host = host.split(':')[0]
                        proxied_url = f"{scheme}://{host}:{port}/proxy/download?url={encoded_url}"
                        logger.info(f"ğŸ”„ ä½¿ç”¨Embyä»£ç†ç«¯å£ç”Ÿæˆä»£ç†URL: {proxied_url[:80]}...")
                    else:
                        # å›é€€ï¼šä½¿ç”¨localhostçš„Embyä»£ç†ç«¯å£
                        proxied_url = f"http://localhost:{port}/proxy/download?url={encoded_url}"
                        logger.info(f"ğŸ”„ å›é€€ä½¿ç”¨localhost Embyä»£ç†ç«¯å£: {proxied_url[:80]}...")
                except Exception as e:
                    # å›é€€ï¼šä½¿ç”¨localhostçš„Embyä»£ç†ç«¯å£
                    proxied_url = f"http://localhost:{port}/proxy/download?url={encoded_url}"
                    logger.info(f"ğŸ”„ åœ°å€æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨localhost Embyä»£ç†ç«¯å£: {e}")
            
            return proxied_url

        except Exception as e:
            logger.error(f"âŒ ä»£ç†ä¸‹è½½é“¾æ¥ç”Ÿæˆå¼‚å¸¸: {e}")
            return download_url
    
    def _quick_validate_direct_url(self, direct_url):
        """å¿«é€ŸéªŒè¯ç›´è¿URLï¼ˆ0.8ç§’è¶…æ—¶ï¼‰"""
        try:
            import requests
            logger.debug(f"ğŸ§ª å¿«é€ŸéªŒè¯ç›´è¿: {direct_url[:60]}...")
            
            # ä½¿ç”¨å¾ˆçŸ­çš„è¶…æ—¶æ—¶é—´è¿›è¡Œå¿«é€ŸéªŒè¯
            response = requests.head(direct_url, timeout=0.8, allow_redirects=False)
            
            # 200, 206, 302, 301éƒ½è®¤ä¸ºæ˜¯æˆåŠŸ
            if response.status_code in [200, 206, 301, 302]:
                logger.debug(f"âœ… ç›´è¿éªŒè¯é€šè¿‡: HTTP {response.status_code}")
                return True
            else:
                logger.debug(f"âš ï¸ ç›´è¿è¿”å›: HTTP {response.status_code}")
                return False
                
        except requests.exceptions.Timeout:
            logger.debug(f"âš ï¸ ç›´è¿éªŒè¯è¶…æ—¶(0.8s)")
            return False
        except Exception as e:
            logger.debug(f"âš ï¸ ç›´è¿éªŒè¯å¼‚å¸¸: {e}")
            return False
    
    def _get_fast_proxied_download_link(self, file_name, mapped_path=None):
        """å¿«é€Ÿè·å–ä»£ç†ä¸‹è½½é“¾æ¥ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ä¼˜å…ˆæ£€æŸ¥æ–‡ä»¶æœç´¢ç¼“å­˜
            search_cache = self._get_cached_file_search(file_name)
            
            if search_cache:
                logger.info(f"ğŸ¯ æ–‡ä»¶æœç´¢ç¼“å­˜å‘½ä¸­: {file_name}")
                file_id = search_cache['file_id']
                
                # ç›´æ¥è·å–ä¸‹è½½é“¾æ¥ï¼Œè·³è¿‡æœç´¢æ­¥éª¤
                download_url = self.client.download_url({'FileID': file_id})
                
                if download_url:
                    proxied_url = self._proxy_download_url(download_url)
                    
                    return {
                        'name': file_name,
                        'size': search_cache.get('file_size', 0),
                        'is_dir': False,
                        'modified': search_cache.get('created_time', ''),
                        'raw_url': proxied_url,
                        'sign': '',
                        'header': {}
                    }
            
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œæ‰§è¡Œæœç´¢å¹¶ç¼“å­˜ç»“æœ
            logger.info(f"ğŸ” æ‰§è¡Œ123ç½‘ç›˜æœç´¢: {file_name}")
            
            search_result = self.client.fs_list_new({
                'SearchData': file_name,
                'limit': 10
            })
            
            if search_result and search_result.get('code') == 0:
                items = search_result.get('data', {}).get('InfoList', [])
                
                # æŸ¥æ‰¾ç²¾ç¡®åŒ¹é…å¹¶ç¼“å­˜
                for item in items:
                    if item.get('FileName') == file_name and item.get('Type') == 0:
                        file_id = item['FileId']
                        
                        # ç¼“å­˜æœç´¢ç»“æœï¼ˆ1å°æ—¶ï¼‰
                        self._cache_file_search(file_name, {
                            'file_id': file_id,
                            'file_size': item.get('Size', 0),
                            'created_time': item.get('CreateAt', ''),
                            'parent_id': item.get('ParentFileId', '')
                        })
                        
                        logger.info(f"ğŸ“ æ–‡ä»¶æœç´¢ç»“æœå·²ç¼“å­˜: {file_name}")
                        
                        # è·å–ä¸‹è½½é“¾æ¥
                        download_url = self.client.download_url({'FileID': file_id})
                        
                        if download_url:
                            proxied_url = self._proxy_download_url(download_url)
                            
                            return {
                                'name': file_name,
                                'size': item.get('Size', 0),
                                'is_dir': False,
                                'modified': item.get('CreateAt', ''),
                                'raw_url': proxied_url,
                                'sign': '',
                                'header': {}
                            }
                        break
            
            logger.warning(f"âš ï¸ å¿«é€Ÿä»£ç†è·å–å¤±è´¥: {file_name}")
            return None
            
        except Exception as e:
            logger.error(f"âŒ å¿«é€Ÿä»£ç†ä¸‹è½½å¼‚å¸¸: {e}")
            return None
    
    def _get_cached_file_search(self, file_name):
        """è·å–ç¼“å­˜çš„æ–‡ä»¶æœç´¢ç»“æœ"""
        try:
            from database.database import get_db_manager
            db = get_db_manager()
            return db.get_file_search_cache(file_name)
        except Exception:
            return None
    
    def _cache_file_search(self, file_name, file_info):
        """ç¼“å­˜æ–‡ä»¶æœç´¢ç»“æœ"""
        try:
            from database.database import get_db_manager
            db = get_db_manager()
            
            db.set_file_search_cache(
                filename=file_name,
                file_id=file_info['file_id'],
                file_size=file_info.get('file_size'),
                parent_id=file_info.get('parent_id'),
                created_time=file_info.get('created_time'),
                expire_seconds=3600  # 1å°æ—¶ç¼“å­˜
            )
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æœç´¢å¤±è´¥: {e}")
            return False

    def _can_build_from_domain_path(self):
        """åˆ¤æ–­æ˜¯å¦å¯é€šè¿‡è‡ªå®šä¹‰åŸŸå + è·¯å¾„ç›´å‡º"""
        try:
            auth_cfg = self.config.get('123', {}).get('url_auth', {})
            domains = auth_cfg.get('custom_domains', []) or []
            return bool(auth_cfg.get('enable')) and len(domains) > 0
        except Exception:
            return False

    def _build_url_from_domain_and_path(self, mapped_path):
        """ä½¿ç”¨è‡ªå®šä¹‰åŸŸå + ç½‘ç›˜æ˜ å°„è·¯å¾„ç›´æ¥æ„é€ ç›´é“¾URL"""
        try:
            # mapped_path å½¢å¦‚: /123/dy/... éœ€å»é™¤æŒ‚è½½å‰ç¼€
            mount = (self.config.get('123', {}) or {}).get('mount_path', '/123')
            path_part = mapped_path
            if mount and mapped_path.startswith(mount):
                path_part = mapped_path[len(mount):]
            if not path_part.startswith('/'):
                path_part = '/' + path_part

            auth_cfg = self.config.get('123', {}).get('url_auth', {})
            domains = auth_cfg.get('custom_domains', []) or []
            if not domains:
                return None
            domain = domains[0].strip().rstrip('/')
            if domain.startswith('http://') or domain.startswith('https://'):
                base = domain
            else:
                base = f"https://{domain}"
            return f"{base}{path_part}"
        except Exception:
            return None
    
    def _search_file(self, file_name):
        """æœç´¢æ–‡ä»¶è·å–FileID"""
        try:
            logger.info(f"ğŸ” æœç´¢æ–‡ä»¶: {file_name}")
            
            search_result = self.client.fs_list_new({
                'SearchData': file_name,
                'limit': 20
            })
            
            if not search_result or search_result.get('code') != 0:
                logger.warning(f"âš ï¸ æœç´¢APIè¿”å›å¼‚å¸¸")
                return None, None
            
            items = search_result.get('data', {}).get('InfoList', [])
            logger.info(f"  æ‰¾åˆ° {len(items)} ä¸ªç»“æœ")
            
            # æŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„æ–‡ä»¶
            for item in items:
                if item.get('FileName') == file_name and item.get('Type') == 0:
                    file_id = item['FileId']
                    logger.info(f"âœ… åŒ¹é…æ–‡ä»¶: FileId={file_id}")
                    return file_id, item
            
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„æ–‡ä»¶")
            return None, None
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¼‚å¸¸: {e}")
            return None, None
    
    # å·²ç§»é™¤ï¼šOpenAPI ä¸ download_url è·å–ç›´é“¾çš„å®ç°
    
    def _add_url_auth(self, url):
        """æ·»åŠ URLé‰´æƒç­¾å"""
        if not url:
            return url
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é‰´æƒ
        auth_config = self.config.get('123', {}).get('url_auth', {})
        if not auth_config.get('enable'):
            logger.info(f"  URLé‰´æƒæœªå¯ç”¨")
            return url
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‰´æƒï¼ˆæ”¯æŒè‡ªå®šä¹‰åŸŸåï¼‰
        custom_domains = auth_config.get('custom_domains', [])
        if not self.auth_manager.is_123pan_url(url, custom_domains):
            logger.info(f"  é123ç½‘ç›˜URLï¼Œè·³è¿‡é‰´æƒ")
            return url
        
        # è·å–é‰´æƒå‚æ•°
        secret_key = auth_config.get('secret_key')
        uid = auth_config.get('uid')
        expire_time = auth_config.get('expire_time', 3600)
        
        if not (secret_key and uid):
            logger.warning(f"âš ï¸ URLé‰´æƒå·²å¯ç”¨ä½†ç¼ºå°‘é…ç½®")
            return url
        
        # æ·»åŠ ç­¾å
        logger.debug(f"ğŸ” æ·»åŠ URLé‰´æƒç­¾å...")
        auth_url = self.auth_manager.add_auth_to_url(url, secret_key, uid, expire_time)
        
        return auth_url
    
    # å·²ç§»é™¤ï¼šè·å– OpenAPI access_token çš„é€»è¾‘

